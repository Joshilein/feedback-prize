{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:16.330264Z",
     "iopub.status.busy": "2025-06-28T07:55:16.329880Z",
     "iopub.status.idle": "2025-06-28T07:55:31.472203Z",
     "shell.execute_reply": "2025-06-28T07:55:31.471413Z",
     "shell.execute_reply.started": "2025-06-28T07:55:16.330244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import spacy.util\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, words as nltk_words\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tqdm import tqdm\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "trans = preprocessing.MinMaxScaler()\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:31.474264Z",
     "iopub.status.busy": "2025-06-28T07:55:31.473660Z",
     "iopub.status.idle": "2025-06-28T07:55:38.147042Z",
     "shell.execute_reply": "2025-06-28T07:55:38.146505Z",
     "shell.execute_reply.started": "2025-06-28T07:55:31.474243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#nlp = spacy.load(\"/kaggle/input/en-core-web-lg/en_core_web_lg/en_core_web_lg-3.8.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.148057Z",
     "iopub.status.busy": "2025-06-28T07:55:38.147800Z",
     "iopub.status.idle": "2025-06-28T07:55:38.151813Z",
     "shell.execute_reply": "2025-06-28T07:55:38.150973Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.148035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "download_dir=\"./.venv/nltk_data\"\n",
    "#download_dir=\"/root/nltk_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.154006Z",
     "iopub.status.busy": "2025-06-28T07:55:38.153778Z",
     "iopub.status.idle": "2025-06-28T07:55:38.432854Z",
     "shell.execute_reply": "2025-06-28T07:55:38.432311Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.153992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "full_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cohesion",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "syntax",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "vocabulary",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "phraseology",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "grammar",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "conventions",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "6a54440f-520e-4250-9528-b1bb7975264c",
       "rows": [
        [
         "0",
         "0016926B079C",
         "I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\n\nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear.\n\nmost students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.\n\nwhen your home your comfortable and you pay attention. it gives then an advantage to be smarter and even pass there classmates on class work. public schools are difficult even if you try. some teacher dont know how to teach it in then way that students understand it. that causes students to fail and they may repeat the class.              ",
         "3.5",
         "3.5",
         "3.0",
         "3.0",
         "4.0",
         "3.0"
        ],
        [
         "1",
         "0022683E9EA5",
         "When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A\n\nproblem is a change for you because it can make you see different and help you to understand how tings wok.\n\nFirst of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me because everything was different that my country and then i realist that wrong because a problem may change you but sometimes can not change the way it is, but i remember that i was really shy but i think that change a lot because sometimes my problems make me think that there is more thing that i never see in my life but i just need to see it from a different way and dont let nothing happened and ruing the change that i want to make because of just a problem. For example i think that nothing was going to change me and that i dont need to be shy anymore became i need to start seeing everything in a different ways because you can get mad at every one but you need to know what is going to happened after,\n\npeople may see you different but the only way that you know how to change is to do the best and don't let nothing or not body to change nothing about you. The way you want to change not one have that and can't do nothing about it because is your choice and your problems and you can decide what to do with it.\n\nsecond of all can help you to understand how things work. For instance my mom have a lot of problems but she have faith when she is around people, my mom is scare of high and i'm not scare of high i did not understand why my mos is scare of high and in not scare of high and every time i see my mom in a airplane it make me laugh because she is scare and is funny, but i see it from a different way and i like the high but also she have to understand that hoe things work in other people because it can no be the same as you. For example i think that my mom and me are different because we are and i have to understand that she does not like high and i need to understand that. to help someone to understand how things work you need to start to see how things work in that persons life.\n\nA problem is a change for you and can make you a different and help you to understand. Everyone has a different opinion and a different was to understand then others. everyone can see the different opinion and what other people think.",
         "2.5",
         "2.5",
         "3.0",
         "2.0",
         "2.0",
         "2.5"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train=pd.read_csv('data/train.csv')\n",
    "test=pd.read_csv('data/test.csv')\n",
    "from tqdm import tqdm\n",
    "# IN KAGGLE:\n",
    "# train = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/train.csv\")\n",
    "# test = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/test.csv\")\n",
    "display(train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.433629Z",
     "iopub.status.busy": "2025-06-28T07:55:38.433455Z",
     "iopub.status.idle": "2025-06-28T07:55:38.437260Z",
     "shell.execute_reply": "2025-06-28T07:55:38.436534Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.433615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.438116Z",
     "iopub.status.busy": "2025-06-28T07:55:38.437914Z",
     "iopub.status.idle": "2025-06-28T07:55:38.576341Z",
     "shell.execute_reply": "2025-06-28T07:55:38.575863Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.438093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'n_capital'\n",
    "train[var]=train['full_text'].apply(count_capital_words)\n",
    "test[var]=test['full_text'].apply(count_capital_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.577037Z",
     "iopub.status.busy": "2025-06-28T07:55:38.576881Z",
     "iopub.status.idle": "2025-06-28T07:55:38.580927Z",
     "shell.execute_reply": "2025-06-28T07:55:38.580311Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.577024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_punctuations(text):\n",
    "    punctuations=\"'!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~'\"\n",
    "    count=0\n",
    "    for i in punctuations:\n",
    "        count+=text.count(i)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.581805Z",
     "iopub.status.busy": "2025-06-28T07:55:38.581544Z",
     "iopub.status.idle": "2025-06-28T07:55:38.721012Z",
     "shell.execute_reply": "2025-06-28T07:55:38.720555Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.581783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'n_punct'\n",
    "train[var]=train['full_text'].apply(count_punctuations)\n",
    "test[var]=test['full_text'].apply(count_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.721683Z",
     "iopub.status.busy": "2025-06-28T07:55:38.721525Z",
     "iopub.status.idle": "2025-06-28T07:55:38.725344Z",
     "shell.execute_reply": "2025-06-28T07:55:38.724764Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.721670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def n_unique_words(text):\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text.lower()) \n",
    "    return len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:38.727778Z",
     "iopub.status.busy": "2025-06-28T07:55:38.727516Z",
     "iopub.status.idle": "2025-06-28T07:55:39.284368Z",
     "shell.execute_reply": "2025-06-28T07:55:39.283834Z",
     "shell.execute_reply.started": "2025-06-28T07:55:38.727758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'n_unique'\n",
    "train[var]=train['full_text'].apply(n_unique_words)\n",
    "test[var]=test['full_text'].apply(n_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Words (exclude stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:39.285146Z",
     "iopub.status.busy": "2025-06-28T07:55:39.284934Z",
     "iopub.status.idle": "2025-06-28T07:55:39.289582Z",
     "shell.execute_reply": "2025-06-28T07:55:39.288834Z",
     "shell.execute_reply.started": "2025-06-28T07:55:39.285130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def n_unique_words_no_stop(text):\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    unique_words = np.unique(words)\n",
    "    return len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:39.290651Z",
     "iopub.status.busy": "2025-06-28T07:55:39.290360Z",
     "iopub.status.idle": "2025-06-28T07:55:40.589350Z",
     "shell.execute_reply": "2025-06-28T07:55:40.588591Z",
     "shell.execute_reply.started": "2025-06-28T07:55:39.290635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'n_unique_n_stop'\n",
    "train[var]=train['full_text'].apply(n_unique_words_no_stop)\n",
    "test[var]=test['full_text'].apply(n_unique_words_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:40.590253Z",
     "iopub.status.busy": "2025-06-28T07:55:40.590032Z",
     "iopub.status.idle": "2025-06-28T07:55:40.594874Z",
     "shell.execute_reply": "2025-06-28T07:55:40.594120Z",
     "shell.execute_reply.started": "2025-06-28T07:55:40.590237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def n_non_words(text):\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    english_words = set(nltk_words.words())\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    non_words = [t for t in tokens if t not in stop_words and t not in english_words]\n",
    "    return len(set(non_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T07:55:40.595879Z",
     "iopub.status.busy": "2025-06-28T07:55:40.595614Z",
     "iopub.status.idle": "2025-06-28T08:00:36.133226Z",
     "shell.execute_reply": "2025-06-28T08:00:36.132455Z",
     "shell.execute_reply.started": "2025-06-28T07:55:40.595865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'n_n_word'\n",
    "train[var]=train['full_text'].apply(n_non_words)\n",
    "test[var]=test['full_text'].apply(n_non_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:00:36.134280Z",
     "iopub.status.busy": "2025-06-28T08:00:36.134046Z",
     "iopub.status.idle": "2025-06-28T08:00:36.138041Z",
     "shell.execute_reply": "2025-06-28T08:00:36.137353Z",
     "shell.execute_reply.started": "2025-06-28T08:00:36.134263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_noun_phrases(text):\n",
    "    blob = TextBlob(text)\n",
    "    return len(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:00:36.139093Z",
     "iopub.status.busy": "2025-06-28T08:00:36.138810Z",
     "iopub.status.idle": "2025-06-28T08:00:54.722402Z",
     "shell.execute_reply": "2025-06-28T08:00:54.721583Z",
     "shell.execute_reply.started": "2025-06-28T08:00:36.139051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'noun_phrase_count'\n",
    "train[var]=train['full_text'].apply(count_noun_phrases)\n",
    "test[var]=test['full_text'].apply(count_noun_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:00:54.723543Z",
     "iopub.status.busy": "2025-06-28T08:00:54.723280Z",
     "iopub.status.idle": "2025-06-28T08:00:54.728096Z",
     "shell.execute_reply": "2025-06-28T08:00:54.727318Z",
     "shell.execute_reply.started": "2025-06-28T08:00:54.723516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pos(text):\n",
    "    doc = nlp(text)\n",
    "    result = dict(Counter([t.pos_ for t in doc]))\n",
    "    missing = set(var) - set(result.keys())\n",
    "    for miss in missing:\n",
    "        result[miss] = 0.0\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T08:00:54.729529Z",
     "iopub.status.busy": "2025-06-28T08:00:54.728846Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting POS\n",
      "Finished POS\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting POS\")\n",
    "var = ['PRON', 'VERB',\t'SCONJ', 'NOUN', 'AUX', 'ADP', 'PUNCT', 'PART',\t'CCONJ', 'ADV', 'DET', 'ADJ', 'SPACE', 'PROPN', 'NUM', 'INTJ', 'SYM', 'X']\n",
    "train[var]=train['full_text'].apply(pos)\n",
    "test[var]=test['full_text'].apply(pos)\n",
    "print(\"Finished POS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def essay_polarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'polarity'\n",
    "train[var]=train['full_text'].apply(essay_polarity)\n",
    "test[var]=test['full_text'].apply(essay_polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essay Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def essay_subjectivity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "var = 'subjectivity'\n",
    "train[var]=train['full_text'].apply(essay_subjectivity)\n",
    "test[var]=test['full_text'].apply(essay_subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spellchecker\n",
    "score: 0 (no mistake) - 1 (all wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def spell_similarity(text): \n",
    "    b = nlp(TextBlob(text).correct().string)\n",
    "    return nlp(text).similarity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spellchecker\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Spellchecker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspell_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspell_similarity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspell_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(spell_similarity)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Spellchecker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/pandas/core/series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m, in \u001b[0;36mspell_similarity\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mspell_similarity\u001b[39m(text): \n\u001b[0;32m----> 2\u001b[0m     b \u001b[38;5;241m=\u001b[39m nlp(\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstring)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nlp(text)\u001b[38;5;241m.\u001b[39msimilarity(b)\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/blob.py:555\u001b[0m, in \u001b[0;36mBaseBlob.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    554\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (Word(w)\u001b[38;5;241m.\u001b[39mcorrect() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m--> 555\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/blob.py:554\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# regex matches: word or punctuation or whitespace\u001b[39;00m\n\u001b[1;32m    553\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mtokenize\u001b[38;5;241m.\u001b[39mregexp_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 554\u001b[0m corrected \u001b[38;5;241m=\u001b[39m (\u001b[43mWord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[1;32m    555\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(corrected)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(ret)\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/blob.py:115\u001b[0m, in \u001b[0;36mWord.correct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcorrect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Word(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspellcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/blob.py:107\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mspellcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/en/__init__.py:118\u001b[0m, in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msuggest\u001b[39m(w):\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of (word, confidence)-tuples of spelling corrections.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/_text.py:1692\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misdigit():\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [(w, \u001b[38;5;241m1.0\u001b[39m)]  \u001b[38;5;66;03m# 1.5\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m candidates \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known([w])\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w))\n\u001b[0;32m-> 1692\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m [w]\n\u001b[1;32m   1694\u001b[0m )\n\u001b[1;32m   1695\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0.0\u001b[39m), c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m   1696\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p, word \u001b[38;5;129;01min\u001b[39;00m candidates) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/_text.py:1667\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_edit1\u001b[49m\u001b[43m(\u001b[49m\u001b[43me1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/_text.py:1667\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(e2 \u001b[38;5;28;01mfor\u001b[39;00m e1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(w) \u001b[38;5;28;01mfor\u001b[39;00m e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edit1(e1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43me2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/_text.py:104\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__contains__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/feedback-prize/.venv/lib/python3.9/site-packages/textblob/_text.py:92\u001b[0m, in \u001b[0;36mlazydict._lazy\u001b[0;34m(self, method, *args)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, types\u001b[38;5;241m.\u001b[39mMethodType(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mdict\u001b[39m, method), \u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m(\u001b[38;5;28mdict\u001b[39m, method)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "print(\"Starting Spellchecker\")\n",
    "train['spell_score'] = train['full_text'].apply(spell_similarity)\n",
    "test['spell_score'] = test['full_text'].apply(spell_similarity)\n",
    "print(\"Finished Spellchecker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sentence_lengths(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    lengths = [n_unique_words(sent) for sent in sentences if sent.strip()]\n",
    "    return lengths if lengths else [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sentence_av_len_calc(text):\n",
    "    return np.mean(sentence_lengths(text))\n",
    "\n",
    "train['av_sent_len']=train['full_text'].apply(sentence_av_len_calc)\n",
    "test['av_sent_len']=test['full_text'].apply(sentence_av_len_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sentence_max_len_calc(text):\n",
    "    return np.max(sentence_lengths(text))\n",
    "\n",
    "train['max_sent_len']=train['full_text'].apply(sentence_max_len_calc)\n",
    "test['max_sent_len']=test['full_text'].apply(sentence_max_len_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sentence_min_len_calc(text):\n",
    "    return np.min(sentence_lengths(text))\n",
    "\n",
    "train['min_sent_len']=train['full_text'].apply(sentence_min_len_calc)\n",
    "test['min_sent_len']=test['full_text'].apply(sentence_min_len_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sentence_median_len_calc(text):\n",
    "    return np.median(sentence_lengths(text))\n",
    "\n",
    "train['med_sent_len']=train['full_text'].apply(sentence_median_len_calc)\n",
    "test['med_sent_len']=test['full_text'].apply(sentence_median_len_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sentence_std_len_calc(text):\n",
    "    return np.std(sentence_lengths(text))\n",
    "\n",
    "train['std_sent_len']=train['full_text'].apply(sentence_std_len_calc)\n",
    "test['std_sent_len']=test['full_text'].apply(sentence_std_len_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_sentences(text):\n",
    "    return len(nltk.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train['num_sent']= train[\"full_text\"].apply(count_sentences)\n",
    "test['num_sent']=test[\"full_text\"].apply(count_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon', download_dir=download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentiment scores: –4 (very negativ) and +4 (very positiv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_sentiment_scores(data):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    neg=[]\n",
    "    pos=[]\n",
    "    neu=[]\n",
    "    comp=[]\n",
    "    for sentence in tqdm(data['full_text'].values): \n",
    "        sentence_sentiment_score = sid.polarity_scores(sentence)\n",
    "        comp.append(sentence_sentiment_score['compound'])\n",
    "        neg.append(sentence_sentiment_score['neg'])\n",
    "        pos.append(sentence_sentiment_score['pos'])\n",
    "        neu.append(sentence_sentiment_score['neu'])\n",
    "    return comp,neg,pos,neu\n",
    "train['compound'],train['negative'],train['positive'],train['neutral']=generate_sentiment_scores(train)\n",
    "test['compound'],test['negative'],test['positive'],test['neutral']=generate_sentiment_scores(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(train[['full_text', 'compound', 'negative', 'positive', 'neutral']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train['char_len']=train['full_text'].apply(lambda x:len(x.split()))\n",
    "test['char_len']=test['full_text'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_train = pd.DataFrame()\n",
    "processed_test = pd.DataFrame()\n",
    "processed_train[\"tokens\"] = train[\"full_text\"].apply(lambda s: word_tokenize(s))\n",
    "processed_test[\"tokens\"] = test[\"full_text\"].apply(lambda s: word_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different words contains the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(tokens):\n",
    "    return len(set(tokens)) / max(len(tokens), 1)\n",
    "\n",
    "train[\"lex_diversity\"] = processed_train[\"tokens\"].apply(lexical_diversity)\n",
    "test[\"lex_diversity\"] = processed_test[\"tokens\"].apply(lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_features=10_000,\n",
    "    min_df=5\n",
    ")\n",
    "X_tfidf_train = tfidf.fit_transform(train['full_text'])\n",
    "X_tfidf_test = tfidf.transform(test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tfidf_means = np.asarray(X_tfidf_train.mean(axis=0)).flatten()\n",
    "tfidf_features = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "tfidf_df_train = pd.DataFrame(X_tfidf_train.toarray(), columns=tfidf_features, index=train.index)\n",
    "tfidf_df_test = pd.DataFrame(X_tfidf_test.toarray(), columns=tfidf_features, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Concatenate to original DataFrames\n",
    "train = pd.concat([train.reset_index(drop=True), tfidf_df_train.reset_index(drop=True)], axis=1)\n",
    "test = pd.concat([test.reset_index(drop=True), tfidf_df_test.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train=train[['cohesion','syntax','vocabulary','phraseology','grammar','conventions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load feature engineered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('data/feature_engineered/train_vars_test.csv', index=False)\n",
    "y_train.to_csv('data/feature_engineered/y_vars_test.csv', index=False)\n",
    "test.to_csv('data/feature_engineered/test_vars_test.csv', index=False)\n",
    "#train.to_csv('/kaggle/working/train_vars.csv', index=False)\n",
    "#y_train.to_csv('/kaggle/working/train_y_vars.csv', index=False)\n",
    "#test.to_csv('/kaggle/working/test_vars.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display(train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/feature_engineered/train_vars_test.csv\")\n",
    "df_test = pd.read_csv(\"data/feature_engineered/test_vars_test.csv\")\n",
    "#df_train = pd.read_csv(\"/kaggle/working/train_vars.csv\")\n",
    "#df_test = pd.read_csv(\"/kaggle/working/test_vars.csv\")\n",
    "display(df_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_cols = df_train.columns.difference(['text_id', 'full_text'] + label_cols).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MCRMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        colwise_mse = self.mse(preds, targets).mean(dim=0)\n",
    "        colwise_rmse = torch.sqrt(colwise_mse + 1e-8)\n",
    "        return colwise_rmse.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df_train[feature_cols]\n",
    "y_all = df_train[label_cols]\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# convert in torch.Tensor \n",
    "y_pred_tensor = torch.tensor(y_pred, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32) \n",
    "\n",
    "# calculate MCRMSE \n",
    "mcrmse_loss = MCRMSELoss()\n",
    "loss_value = mcrmse_loss(y_pred_tensor, y_test_tensor)\n",
    "\n",
    "print(f\"Evaluation mit MCRMSE: {loss_value.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X_submission = df_test[feature_cols]\n",
    "\n",
    "# submission_preds = model.predict(X_submission)\n",
    "\n",
    "# submission_df = pd.DataFrame(submission_preds, columns=label_cols)\n",
    "# submission_df[\"text_id\"] = df_test[\"text_id\"] \n",
    "# submission_df = submission_df[[\"text_id\"] + label_cols]  \n",
    "\n",
    "# submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 4196674,
     "sourceId": 38321,
     "sourceType": "competition"
    },
    {
     "datasetId": 7600487,
     "sourceId": 12074281,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
